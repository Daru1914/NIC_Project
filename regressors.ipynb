{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "# import models\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import RANSACRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "# import metrics and scoring modules\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_squared_log_error\n",
    "# import tuning modules\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "X, Y = make_regression(n_samples=4000, n_features=20, random_state=18, n_informative=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_function = {\"mse\": mean_squared_error, \"mae\": mean_absolute_error, \"r2\": r2_score, 'rmse':\n",
    "    lambda y_true, y_pred: mean_squared_error(y_true, y_pred, squared=False), \"msle\": mean_squared_log_error}\n",
    "\n",
    "name_score_mapper = {\"mae\": \"neg_mean_absolute_error\", \"mse\": \"neg_mean_squared_error\",\n",
    "                     \"r2\": \"r2\", \"msle\": \"neg_mean_squared_log_error\"}\n",
    "\n",
    "# constant used for cross validation\n",
    "CV = KFold(n_splits=5, shuffle=True, random_state=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_model(model, params_grid, X_train, y_train, cv=None, scoring='neg_mean_absolute_error'):\n",
    "    # if a user-friendly name is given, map it to the official one used by sklearn\n",
    "    if scoring in name_score_mapper:\n",
    "        scoring = name_score_mapper[scoring]\n",
    "\n",
    "    if cv is None:\n",
    "        cv = CV\n",
    "\n",
    "    searcher = GridSearchCV(model, param_grid=params_grid, cv=cv, scoring=scoring, n_jobs=-1)\n",
    "    searcher.fit(X_train, y_train)\n",
    "    return searcher.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_tuned_model(tuned_model, X_train, X_test, y_train, y_test, train=True, metrics=None):\n",
    "    # set the default metric\n",
    "    if metrics is None:\n",
    "        metrics = ['mse']\n",
    "\n",
    "    if isinstance(metrics, str):\n",
    "        metrics = [metrics]\n",
    "\n",
    "    if 'msle' in metrics and (y_train <= 0).any():\n",
    "        # msle cannot be used for target variables with non-positive values\n",
    "        metrics.remove('msle')\n",
    "\n",
    "    # train the model\n",
    "    if train:\n",
    "        tuned_model.fit(X_train, y_train)\n",
    "\n",
    "    # predict on the test dataset\n",
    "    y_pred = tuned_model.predict(X_test)\n",
    "    # evaluate the model\n",
    "    scores = dict(list(zip(metrics, [score_function[m](y_test, y_pred) for m in metrics])))\n",
    "    return tuned_model, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(tuned_model, path):\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(tuned_model, f)\n",
    "\n",
    "\n",
    "def load_model(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_model(model, X, y, params_grid, save=True, save_path=None, test_size=0.2, tune_metric=None,\n",
    "              test_metrics=None, cv=None):\n",
    "    # the dataset passed is assumed to be ready to be processed\n",
    "    # all its features are numerical and all its missing values are imputed/discarded\n",
    "\n",
    "    if save and save_path is None:\n",
    "        raise ValueError(\"Please pass a path to save the model or set the 'save' parameter to False\")\n",
    "\n",
    "    # split the dataset into train and test datasets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=11)\n",
    "\n",
    "    # tune the model\n",
    "    tuned_model = tune_model(model, params_grid, X_train, y_train, cv=cv, scoring=tune_metric)\n",
    "\n",
    "    # evaluate the tuned model\n",
    "    model, results = evaluate_tuned_model(tuned_model, X_train, X_test, y_train, y_test, metrics=test_metrics)\n",
    "    # save the model to the passed path\n",
    "    if save:\n",
    "        save_model(tuned_model, save_path)\n",
    "\n",
    "    return model, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mse': 0.003949899818051906, 'rmse': 0.0628482284400436, 'r2': 0.9999998998382846}\n"
     ]
    }
   ],
   "source": [
    "ridge_basic = Ridge(max_iter=5000)\n",
    "\n",
    "ridge_grid = {\"alpha\": np.logspace(0.001, 10, 20)}\n",
    "\n",
    "def try_ridge(X, y, lr_model=ridge_basic, params_grid=None, save=True, save_path=None,\n",
    "              test_size=0.2, tune_metric=None, test_metrics=None, cv=None):\n",
    "    if params_grid is None:\n",
    "        params_grid = ridge_grid\n",
    "\n",
    "    return try_model(lr_model, X, y, params_grid, save=save, save_path=save_path,\n",
    "                     test_size=test_size, tune_metric=tune_metric, test_metrics=test_metrics, cv=cv)\n",
    "\n",
    "X, Y = make_regression(n_samples=4000, n_features=20, random_state=18, n_informative=8)\n",
    "\n",
    "lr, results = try_ridge(X, Y, save=False, test_metrics=['mse', 'rmse', 'r2', \"msle\"], tune_metric='mse')\n",
    "\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mse': 19199.67985795589, 'rmse': 138.5629093875987, 'r2': 0.5131337608205551}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "11600 fits failed out of a total of 22040.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "870 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1342, in fit\n",
      "    super().fit(\n",
      "  File \"c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 265, in fit\n",
      "    check_scalar(\n",
      "  File \"c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 1480, in check_scalar\n",
      "    raise ValueError(\n",
      "ValueError: min_samples_split == 1, must be >= 2.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5220 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1342, in fit\n",
      "    super().fit(\n",
      "  File \"c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 394, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_, n_samples)\n",
      "                ~~~~~~~~~~~~^^^^^^^^^^^^^^^^\n",
      "KeyError: 'squared error'\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5510 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1342, in fit\n",
      "    super().fit(\n",
      "  File \"c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 185, in fit\n",
      "    raise ValueError(\n",
      "ValueError: Some value(s) of y are negative which is not allowed for Poisson regression.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:953: UserWarning: One or more of the test scores are non-finite: [nan nan nan ... nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dtr_regressor = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "dtr_grid = {'criterion': ['squared error', 'friedman_mse', 'absolute_error', 'poisson'],\n",
    "            'splitter':['random', 'best'], 'max_depth':list(range(1, 30)),\n",
    "            'min_samples_split': list(range(1, 20))}\n",
    "\n",
    "def try_dtr(X, y, dtr_model=dtr_regressor, params_grid=None, save=True, save_path=None,\n",
    "              test_size=0.2, tune_metric=None, test_metrics=None, cv=None):\n",
    "    if params_grid is None:\n",
    "        params_grid = dtr_grid\n",
    "\n",
    "    return try_model(dtr_model, X, y, params_grid, save=save, save_path=save_path,\n",
    "                     test_size=test_size, tune_metric=tune_metric, test_metrics=test_metrics, cv=cv)\n",
    "\n",
    "lr, results = try_dtr(X, Y, save=False, test_metrics=['mse', 'rmse', 'r2', \"msle\"], tune_metric='mse')\n",
    "\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KN Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mse': 11501.592511643237, 'rmse': 107.24547781441993, 'r2': 0.7083421634034271}\n"
     ]
    }
   ],
   "source": [
    "knn_regressor = KNeighborsRegressor()\n",
    "\n",
    "knn_grid = {'n_neighbors': list(range(1, 21)), 'weights': ['uniform', 'distance'],\n",
    "            'p': [1, 2, 3]}\n",
    "\n",
    "def try_knn(X, y, knn_model=knn_regressor, params_grid=None, save=True, save_path=None,\n",
    "              test_size=0.2, tune_metric=None, test_metrics=None, cv=None):\n",
    "    if params_grid is None:\n",
    "        params_grid = knn_grid\n",
    "\n",
    "    return try_model(knn_model, X, y, params_grid, save=save, save_path=save_path,\n",
    "                     test_size=test_size, tune_metric=tune_metric, test_metrics=test_metrics, cv=cv)\n",
    "\n",
    "lr, results = try_knn(X, Y, save=False, test_metrics=['mse', 'rmse', 'r2', \"msle\"], tune_metric='mse')\n",
    "\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mse': 5.598142835038566e-26, 'rmse': 2.366039482983867e-13, 'r2': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "linear_regressor = LinearRegression()\n",
    "\n",
    "linear_grid = {'fit_intercept': [True, False], 'normalize': [True, False], 'copy_X': [True ]}\n",
    "\n",
    "def try_linear(X, y, linear_model=linear_regressor, params_grid=None, save=True, save_path=None,\n",
    "              test_size=0.2, tune_metric=None, test_metrics=None, cv=None):\n",
    "    if params_grid is None:\n",
    "        params_grid = linear_grid\n",
    "\n",
    "    return try_model(linear_model, X, y, params_grid, save=save, save_path=save_path,\n",
    "                     test_size=test_size, tune_metric=tune_metric, test_metrics=test_metrics, cv=cv)\n",
    "\n",
    "lr, results = try_linear(X, Y, save=False, test_metrics=['mse', 'rmse', 'r2', \"msle\"], tune_metric='mse')\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mse': 5.598142835038566e-26, 'rmse': 2.366039482983867e-13, 'r2': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "polynomial_regressor = LinearRegression()\n",
    "\n",
    "polynomial_grid = {'degree': list(range(2, 5)), 'interaction_only': [True, False], \n",
    "                   'include_bias': [True, False], 'order': ['C', 'F']}\n",
    "\n",
    "def try_polynomial(X, y, polynomial_model=polynomial_regressor, params_grid=None, save=True, save_path=None,\n",
    "              test_size=0.2, tune_metric=None, test_metrics=None, cv=None):\n",
    "    if params_grid is None:\n",
    "        params_grid = linear_grid\n",
    "\n",
    "    return try_model(polynomial_model, X, y, params_grid, save=save, save_path=save_path,\n",
    "                     test_size=test_size, tune_metric=tune_metric, test_metrics=test_metrics, cv=cv)\n",
    "\n",
    "lr, results = try_polynomial(X, Y, save=False, test_metrics=['mse', 'rmse', 'r2', \"msle\"], tune_metric='mse')\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mse': 22395.30073803946, 'rmse': 149.65059551515142, 'r2': 0.4320990804904612}\n"
     ]
    }
   ],
   "source": [
    "rfr_regressor = RandomForestRegressor()\n",
    "\n",
    "rfr_grid = {'max_leaf_nodes':[2, 4, 6, 7], 'min_samples_split':[5, 10, 20, 50], 'max_depth': [5,10,15,20],\n",
    "            'max_features':[3, 4, 5], 'n_estimators': [50, 100, 200]}\n",
    "\n",
    "def try_rfr(X, y, rfr_model=rfr_regressor, params_grid=None, save=True, save_path=None,\n",
    "              test_size=0.2, tune_metric=None, test_metrics=None, cv=None):\n",
    "    if params_grid is None:\n",
    "        params_grid = rfr_grid\n",
    "\n",
    "    return try_model(rfr_model, X, y, params_grid, save=save, save_path=save_path,\n",
    "                     test_size=test_size, tune_metric=tune_metric, test_metrics=test_metrics, cv=cv)\n",
    "\n",
    "lr, results = try_rfr(X, Y, save=False, test_metrics=['mse', 'rmse', 'r2', \"msle\"], tune_metric='mse', params_grid=rfr_grid)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear RANSAC regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [34], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m ransac_regressor \u001b[39m=\u001b[39m RANSACRegressor()\n\u001b[1;32m----> 3\u001b[0m ransac_grid \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mestimator\u001b[39m\u001b[39m'\u001b[39m: [LinearRegression], \u001b[39m'\u001b[39m\u001b[39mmin_samples\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mlist\u001b[39m(\u001b[39mrange\u001b[39;49m(\u001b[39m0\u001b[39;49m, \u001b[39m0.5\u001b[39;49m, \u001b[39m0.1\u001b[39;49m)),\n\u001b[0;32m      4\u001b[0m                \u001b[39m'\u001b[39m\u001b[39mstop_probability\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mlist\u001b[39m(\u001b[39mrange\u001b[39m(\u001b[39m0.1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m0.1\u001b[39m))}\n\u001b[0;32m      6\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtry_ransac\u001b[39m(X, y, ransac_model\u001b[39m=\u001b[39mransac_regressor, params_grid\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, save\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, save_path\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m      7\u001b[0m               test_size\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m, tune_metric\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, test_metrics\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, cv\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m      8\u001b[0m     \u001b[39mif\u001b[39;00m params_grid \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mTypeError\u001b[0m: 'float' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "'''\n",
    "ransac_regressor = RANSACRegressor()\n",
    "\n",
    "ransac_grid = {'estimator': [LinearRegression], 'min_samples': list(range(0, 0.5, 0.1)),\n",
    "               'stop_probability': list(range(0.1, 1, 0.1))}\n",
    "\n",
    "def try_ransac(X, y, ransac_model=ransac_regressor, params_grid=None, save=True, save_path=None,\n",
    "              test_size=0.2, tune_metric=None, test_metrics=None, cv=None):\n",
    "    if params_grid is None:\n",
    "        params_grid = ransac_grid\n",
    "\n",
    "    return try_model(ransac_model, X, y, params_grid, save=save, save_path=save_path,\n",
    "                     test_size=test_size, tune_metric=tune_metric, test_metrics=test_metrics, cv=cv)\n",
    "\n",
    "lr, results = try_ransac(X, Y, save=False, test_metrics=['mse', 'rmse', 'r2', \"msle\"], tune_metric='mse', params_grid=ransac_grid)\n",
    "\n",
    "print(results)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear SVM regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_regressor = LinearSVR()\n",
    "\n",
    "svr_grid = {'C': [100, 10, 1.0, 0.1, 0.001], 'loss': ['hinge', 'squared_hinge'],\n",
    "               'dual':[True, False], 'fit_intercept': [True, False]}\n",
    "\n",
    "def try_svr(X, y, svr_model=svr_regressor, params_grid=None, save=True, save_path=None,\n",
    "              test_size=0.2, tune_metric=None, test_metrics=None, cv=None):\n",
    "    if params_grid is None:\n",
    "        params_grid = svr_grid\n",
    "\n",
    "    return try_model(svr_model, X, y, params_grid, save=save, save_path=save_path,\n",
    "                     test_size=test_size, tune_metric=tune_metric, test_metrics=test_metrics, cv=cv)\n",
    "\n",
    "lr, results = try_svr(X, Y, save=False, test_metrics=['mse', 'rmse', 'r2', \"msle\"], tune_metric='mse', params_grid=svr_grid)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_regressor = XGBRegressor()\n",
    "\n",
    "xgb_grid = {'booster': ['gbtree', 'gblinear', 'dart'], 'eta': [0.01, 0.05, 0.1, 0.15, 0.2]}\n",
    "\n",
    "def try_xgb(X, y, xgb_model=xgb_regressor, params_grid=None, save=True, save_path=None,\n",
    "              test_size=0.2, tune_metric=None, test_metrics=None, cv=None):\n",
    "    if params_grid is None:\n",
    "        params_grid = xgb_grid\n",
    "\n",
    "    return try_model(xgb_model, X, y, params_grid, save=save, save_path=save_path,\n",
    "                     test_size=test_size, tune_metric=tune_metric, test_metrics=test_metrics, cv=cv)\n",
    "\n",
    "lr, results = try_xgb(X, Y, save=False, test_metrics=['mse', 'rmse', 'r2', \"msle\"], tune_metric='mse', params_grid=xgb_grid)\n",
    "\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "de47f5c92c0ee6f12a59a5613ac5feff6aab19ddff207ba0b3964cced08c4ccc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
