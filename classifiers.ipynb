{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "This notebook is used to create tools to automate the training, tuning and saving of different sklearn models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary: Imports and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import models\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# import tuning modules\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "\n",
    "# import metrics and scoring modules\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score, balanced_accuracy_score\n",
    "\n",
    "import math \n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import get_scorer_names\n",
    "score_function = {\"accuracy\": accuracy_score, \"precision\": precision_score, \"recall\": recall_score, \n",
    "                  \"f1\": f1_score, \"balanced_accuracy\": balanced_accuracy_score}\n",
    "\n",
    "# constant used for cross validation\n",
    "CV = StratifiedKFold(n_splits=5, shuffle=True, random_state=11)\n",
    "# get_scorer_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Systemizing ML processes\n",
    "In this block we split the most common functionalities used in Machine learning in function block for re-usibility. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = make_classification(n_samples=4000, n_features=20, n_classes=3, random_state = 18, n_informative=8)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_model(model, params_grid, X_train, y_train, cv=None, scoring=None):\n",
    "    # scoring should be determined depending on the nature of the classification problem\n",
    "    if scoring is None:\n",
    "        # if the classification problem is binary\n",
    "        if len(np.unique(y_train)) == 2:\n",
    "            scoring = 'f1'\n",
    "        else:\n",
    "            scoring = 'balanced_accuracy'\n",
    "\n",
    "    if cv is None:\n",
    "        cv = CV\n",
    "\n",
    "    searcher = GridSearchCV(model, param_grid=params_grid, cv=cv, scoring=scoring, n_jobs=-1)\n",
    "    searcher.fit(X_train, y_train)\n",
    "    return searcher.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_tuned_model(tuned_model, X_train, X_test, y_train, y_test, train=True, metrics=None):\n",
    "    if metrics is None:\n",
    "        metrics = ['accuracy']\n",
    "    if isinstance(metrics, str):\n",
    "        metrics = [metrics]\n",
    "\n",
    "    # train the model\n",
    "    if train:\n",
    "        tuned_model.fit(X_train, y_train)\n",
    "\n",
    "    # predict on the test dataset\n",
    "    y_pred = tuned_model.predict(X_test)\n",
    "    # evaluate the model\n",
    "    scores = dict(list(zip(metrics, [score_function[m](y_test, y_pred) for m in metrics])))\n",
    "    return tuned_model, scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(tuned_model, path):\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(tuned_model, f)\n",
    "\n",
    "\n",
    "def load_model(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        return pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "def try_model(model, X, y, params_grid, save=True, save_path=None, test_size=0.2, tune_metric=None,\n",
    "              test_metrics=None, cv=None):\n",
    "    # the dataset passed is assumed to be ready to be processed\n",
    "    # all its features are numerical and all its missing values are imputed/discarded\n",
    "\n",
    "    if test_metrics is None:\n",
    "        test_metrics = ['accuracy']\n",
    "\n",
    "    if save and save_path is None:\n",
    "        raise ValueError(\"Please pass a path to save the model or set the 'save' parameter to False\")\n",
    "\n",
    "    # split the dataset into train and test datasets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=11, stratify=y)\n",
    "\n",
    "    # tune the model\n",
    "    tuned_model = tune_model(model, params_grid, X_train, y_train, cv=cv, scoring=tune_metric)\n",
    "\n",
    "    # evaluate teh tuned model\n",
    "    model, results = evaluate_tuned_model(tuned_model, X_train, X_test, y_train, y_test, test_metrics)\n",
    "    # save the model to the passed path\n",
    "    if save:\n",
    "        save_model(tuned_model, save_path)\n",
    "\n",
    "    return model, results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Machine Learning models\n",
    "In this subsection, we customize the ML processes considered above for the most common Machine Learning models:\n",
    "* Logistic Regression\n",
    "* Linear SVM\n",
    "* DecisionTreeClassifier\n",
    "* RandomForestClassifier\n",
    "* XGBoostClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.66625}\n"
     ]
    }
   ],
   "source": [
    "lr_basic = LogisticRegression(max_iter=5000)\n",
    "# lsvc_basic = LinearSVC(max_iter=5000)\n",
    "\n",
    "LR_grid = {\"C\": np.logspace(-5, 5, 20)}\n",
    "# LSVC_grid = {}\n",
    "\n",
    "def try_LR(X, y, lr_model=lr_basic, params_grid=None, save=True, save_path=None, \n",
    "           test_size=0.2, tune_metric=None, test_metrics=['accuracy'], cv=None):\n",
    "    if params_grid is None:\n",
    "        params_grid = LR_grid\n",
    "    if test_metrics is None:\n",
    "        test_metrics = ['accuracy']\n",
    "    return try_model(lr_model, X, y, params_grid, save=save, \n",
    "    save_path=save_path, test_size=test_size, tune_metric=tune_metric, test_metrics=test_metrics, cv=cv)\n",
    "\n",
    "# def try_LSVM(X, y, lsvc_model = lsvc_basic, params_grid=)\n",
    "    \n",
    "lr, results = try_LR(X, Y, save=False)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "200 fits failed out of a total of 400.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "50 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_classes.py\", line 257, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "                                           ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py\", line 1204, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py\", line 1043, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "50 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_classes.py\", line 257, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "                                           ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py\", line 1204, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py\", line 1043, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "50 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_classes.py\", line 257, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "                                           ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py\", line 1204, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py\", line 1043, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "50 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_classes.py\", line 257, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "                                           ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py\", line 1204, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py\", line 1043, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:953: UserWarning: One or more of the test scores are non-finite: [       nan 0.592572          nan 0.58714881        nan 0.59515397\n",
      "        nan 0.58371678        nan        nan 0.65075362 0.65075362\n",
      "        nan        nan 0.63993064 0.63993064        nan 0.65171018\n",
      "        nan 0.64825552        nan 0.64340566        nan 0.64400241\n",
      "        nan        nan 0.65044063 0.65075362        nan        nan\n",
      " 0.63993064 0.63993064        nan 0.65483571        nan 0.65075362\n",
      "        nan 0.64402436        nan 0.63993064        nan        nan\n",
      " 0.6504421  0.6504421         nan        nan 0.63993064 0.63993064\n",
      "        nan 0.65421266        nan 0.65138254        nan 0.64339838\n",
      "        nan 0.64024363        nan        nan 0.6510681  0.65138254\n",
      "        nan        nan 0.64087253 0.64024363        nan 0.64615682\n",
      "        nan 0.6495295         nan 0.64621392        nan 0.64213328\n",
      "        nan        nan 0.64127337 0.6495295         nan        nan\n",
      " 0.63007896 0.64213328]\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.65625}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "lsvc_model = LinearSVC(max_iter=4000)\n",
    "lsvc_parameters = {'C': [100, 10, 1.0, 0.1, 0.001], 'penalty': ['l1', 'l2'], 'loss': ['hinge', 'squared_hinge'],\n",
    "                   'dual':[True, False], 'fit_intercept': [True, False]}\n",
    "\n",
    "def try_LSVC(X, y, lr_model=lsvc_model, params_grid=None, save=True, save_path=None, \n",
    "           test_size=0.2, tune_metric=None, test_metrics=['accuracy'], cv=None):\n",
    "    if params_grid is None:\n",
    "        params_grid = lsvc_parameters\n",
    "    if test_metrics is None:\n",
    "        test_metrics = ['accuracy']\n",
    "    return try_model(lr_model, X, y, params_grid, save=save, \n",
    "    save_path=save_path, test_size=test_size, tune_metric=tune_metric, test_metrics=test_metrics, cv=cv)\n",
    "\n",
    "lsvc, results = try_LSVC(X, Y, save=False, params_grid = lsvc_parameters)\n",
    "\n",
    "print(results)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "580 fits failed out of a total of 11020.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "580 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 969, in fit\n",
      "    super().fit(\n",
      "  File \"c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 265, in fit\n",
      "    check_scalar(\n",
      "  File \"c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 1480, in check_scalar\n",
      "    raise ValueError(\n",
      "ValueError: min_samples_split == 1, must be >= 2.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:953: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.42384784 ... 0.74934864 0.72900989 0.74778959]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.80625}\n"
     ]
    }
   ],
   "source": [
    "decision_tree_model = DecisionTreeClassifier()\n",
    "decision_tree_parameters = {'criterion': ['gini', 'entropy'], 'splitter':['random', 'best'], 'max_depth':list(range(1, 30)),\n",
    "                            'min_samples_split': list(range(1, 20))}\n",
    "\n",
    "def try_Decision_Tree(X, y, dtc_model=decision_tree_model, params_grid=None, save=True, save_path=None, \n",
    "           test_size=0.2, tune_metric=None, test_metrics=['accuracy'], cv=None):\n",
    "    if params_grid is None:\n",
    "        params_grid = decision_tree_parameters\n",
    "    if test_metrics is None:\n",
    "        test_metrics = ['accuracy']\n",
    "    return try_model(dtc_model, X, y, params_grid, save=save, \n",
    "    save_path=save_path, test_size=test_size, tune_metric=tune_metric, test_metrics=test_metrics, cv=cv)\n",
    "\n",
    "lsvc, results = try_Decision_Tree(X, Y, save=False, params_grid = decision_tree_parameters)\n",
    "\n",
    "print(results)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.74625}\n"
     ]
    }
   ],
   "source": [
    "random_forest_model = RandomForestClassifier()\n",
    "random_forest_parameters = {'max_leaf_nodes':[2, 3, 4, 5, 6, 7], 'min_samples_split':[5, 10, 20, 50], 'max_depth': [5,10,15,20],\n",
    "                            'max_features':[3, 4, 5], 'n_estimators': [50, 100, 200]}\n",
    "\n",
    "def try_RFC(X, y, rfc_model=random_forest_model, params_grid=None, save=True, save_path=None, \n",
    "           test_size=0.2, tune_metric=None, test_metrics=['accuracy'], cv=None):\n",
    "    if params_grid is None:\n",
    "        params_grid = random_forest_parameters\n",
    "    if test_metrics is None:\n",
    "        test_metrics = ['accuracy']\n",
    "    return try_model(rfc_model, X, y, params_grid, save=save, \n",
    "    save_path=save_path, test_size=test_size, tune_metric=tune_metric, test_metrics=test_metrics, cv=cv)\n",
    "\n",
    "lsvc, results = try_RFC(X, Y, save=False, params_grid = random_forest_parameters)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.8925}\n"
     ]
    }
   ],
   "source": [
    "xgboost_model = XGBClassifier()\n",
    "xgboost_parameters = {'booster': ['gbtree', 'gblinear', 'dart'], 'eta': [0.01, 0.05, 0.1, 0.15, 0.2]}\n",
    "\n",
    "def try_xgboost(X, y, xgb_model=xgboost_model, params_grid=None, save=True, save_path=None, \n",
    "           test_size=0.2, tune_metric=None, test_metrics=['accuracy'], cv=None):\n",
    "    if params_grid is None:\n",
    "        params_grid = xgboost_parameters\n",
    "    if test_metrics is None:\n",
    "        test_metrics = ['accuracy']\n",
    "    return try_model(xgboost_model, X, y, params_grid, save=save, \n",
    "    save_path=save_path, test_size=test_size, tune_metric=tune_metric, test_metrics=test_metrics, cv=cv)\n",
    "\n",
    "lsvc, results = try_xgboost(X, Y, save=False, params_grid = xgboost_parameters)\n",
    "\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "de47f5c92c0ee6f12a59a5613ac5feff6aab19ddff207ba0b3964cced08c4ccc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
