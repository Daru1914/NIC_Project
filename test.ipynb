{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Imports and Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_excel(os.path.join('databases', 'final_dataset.xlsx'), usecols=lambda x: 'Unnamed' not in x)\n",
    "dataset.drop(columns=['name', 'code'], inplace=True)\n",
    "y = dataset.pop('pop_growth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prepare_train_test(dataset, y, test_size=0.2, random_state=11):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(dataset, y, test_size=test_size, random_state=random_state)\n",
    "    X_train.reset_index(inplace=True, drop=True)\n",
    "    X_test.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    # for non year columns\n",
    "    standard_scaler = StandardScaler()\n",
    "    # for the year column\n",
    "    mm_scaler = MinMaxScaler()\n",
    "\n",
    "    X_train['year'] = mm_scaler.fit_transform(X_train.loc[:, ['year']]).reshape(-1, )\n",
    "    scaled_year = X_train.pop('year')\n",
    "\n",
    "    X_train = pd.DataFrame(data=standard_scaler.fit_transform(X_train), columns=X_train.columns)\n",
    "    X_train['year'] = scaled_year\n",
    "\n",
    "    X_test['year'] = mm_scaler.fit_transform(X_test.loc[:, ['year']]).reshape(-1, )\n",
    "    scaled_year = X_test.pop('year')\n",
    "\n",
    "    X_test = pd.DataFrame(data=standard_scaler.fit_transform(X_test), columns=X_test.columns)\n",
    "    X_test['year'] = scaled_year\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = prepare_train_test(dataset, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Models and their parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "ridge_basic = Ridge(max_iter=5000)\n",
    "ridge_params = ridge_grid = {\"alpha\": np.logspace(0.001, 10, 20)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mse': 0.17850173688651289}\n"
     ]
    }
   ],
   "source": [
    "### ridge\n",
    "from regressors import apply_model\n",
    "ridge_performance = apply_model(ridge_basic, X_train, X_test, y_train, y_test, ridge_params, save=False)\n",
    "print(ridge_performance[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from math import floor\n",
    "from sklearn.decomposition import PCA\n",
    "from sko.PSO import PSO\n",
    "\n",
    "def display_heat_map(data: pd.DataFrame, title: str):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    sns.heatmap(data, linewidth=1, annot=True)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "class PsoMultiCol:\n",
    "\n",
    "    def set_data(self, df):\n",
    "        self.df = df\n",
    "        # calculate the spearmanr correlation of the dataframe's features\n",
    "        corr = spearmanr(df).correlation\n",
    "        # make sure it is symmetric\n",
    "        corr = (corr + corr.T) / 2\n",
    "        # fill the diagonal with 1s\n",
    "        np.fill_diagonal(corr, 1)\n",
    "        # transform the matrix to a dataframe that represents how similar each feature it is to another\n",
    "        self.dis_matrix = pd.DataFrame(data=(1 - np.abs(corr)), columns=list(df.columns), index=list(df.columns))\n",
    "        # have a dictionary mapping the column's order to its name\n",
    "        self.columns_dict = dict(list(zip(range(len(df.columns)), df.columns)))\n",
    "        # set the number of features for later reference\n",
    "        self.num_feats = len(df.columns)\n",
    "        # save the column names for later reference\n",
    "        self.columns = list(df.columns)\n",
    "\n",
    "    def __init__(self, df: pd.DataFrame = None, max_iter: int = 10, vif_threshold: float = 2.5, epsilon: int = 0.1,\n",
    "                 min_fraction=0.40, max_fraction=0.76, step=0.05):  # add other parameters to the game\n",
    "        self.max_iter = max_iter\n",
    "        self.pso = None\n",
    "        # the value that determine whether columns are multi-collinear or not\n",
    "        self.vif_threshold = vif_threshold\n",
    "        # an epsilon value used in the evaluation function\n",
    "        self.epsilon = epsilon\n",
    "        self.min_fraction = min_fraction\n",
    "        self.max_fraction = max_fraction\n",
    "        self.step = step\n",
    "        self.pso = None\n",
    "        if df is not None:\n",
    "            self.set_data(df)\n",
    "\n",
    "    def _get_vif(self, df=None):\n",
    "        if df is None:\n",
    "            df = self.df\n",
    "\n",
    "        vif = pd.DataFrame()\n",
    "        vif['VIF'] = [variance_inflation_factor(df.values, i) for i in range(len(df.columns))]\n",
    "        vif['variables'] = df.columns\n",
    "        return vif.set_index('variables')\n",
    "\n",
    "    def _get_clusters(self, particle: np.array):\n",
    "        particle_size = len(particle)\n",
    "        discrete_particle = np.array([int(x) for x in particle])\n",
    "        cluster_feats = {}\n",
    "        for i in range(particle_size):\n",
    "            # if the value of the cluster is not in the dictinary, initialize the list\n",
    "            if discrete_particle[i] not in cluster_feats:\n",
    "                cluster_feats[discrete_particle[i]] = []\n",
    "            # the cluster_feats will be a map between numbers representing clusters\n",
    "            # and columns representing\n",
    "            cluster_feats[discrete_particle[i]].append(i)\n",
    "\n",
    "        return cluster_feats\n",
    "\n",
    "    def _cluster_scores(self, cluster_feats: dict):\n",
    "        cluster_names = {}\n",
    "        new_order = []\n",
    "        title = \"\"\n",
    "\n",
    "        # map each cluster to its column names\n",
    "        for c, feats in cluster_feats.items():\n",
    "            cluster_names[c] = [self.columns_dict[i] for i in feats]\n",
    "            new_order.extend(feats)\n",
    "            title += f\"{len(feats)}-\"\n",
    "\n",
    "        # how similar the points are inside a single cluster\n",
    "        inner_cluster_score = 0\n",
    "        for c, names in cluster_feats.items():\n",
    "            inner_cluster_score += (1 + np.exp(self.dis_matrix.iloc[names, names].values.sum())) / np.log(\n",
    "                len(names) + np.exp(1))\n",
    "            # inner_cluster_score += 1 + np.nanmean(self.dis_matrix.loc[names, names])\n",
    "\n",
    "        # new_dist_matrix = self.dis_matrix.loc[new_order[::-1], new_order]\n",
    "        # display_heat_map(new_dist_matrix, title)\n",
    "        return inner_cluster_score  # / inter_cluster_score\n",
    "\n",
    "    def _pso_function(self, particle: np.array):\n",
    "        return self._cluster_scores(self._get_clusters(particle))\n",
    "\n",
    "    def _cluster_pso(self, num_clusters):\n",
    "        # determine the function object to pass to the PSO algorithm\n",
    "        pso_function = lambda x: self._pso_function(x)\n",
    "        # bounds\n",
    "        lower_bound = np.zeros(self.num_feats)\n",
    "        upper_bound = np.full(shape=self.num_feats, fill_value=num_clusters, dtype=\"float\")\n",
    "\n",
    "        pso = PSO(func=pso_function, n_dim=self.num_feats, pop=15, max_iter=self.max_iter, lb=lower_bound,\n",
    "                          ub=upper_bound, c1=1.5, c2=1.5)\n",
    "        pso.run()\n",
    "\n",
    "        x, y = pso.gbest_x, pso.gbest_y\n",
    "\n",
    "        cluster_feats = self._get_clusters(x)\n",
    "\n",
    "        new_order = []\n",
    "        title = \"\"\n",
    "        for _, f in cluster_feats.items():\n",
    "            new_order.extend(f)\n",
    "            title += f'{len(f)}-'\n",
    "\n",
    "        print(y)\n",
    "        new_dist_matrix = self.dis_matrix.loc[new_order[::-1], new_order]\n",
    "        display_heat_map(new_dist_matrix, title)\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    def _find_best_cluster(self):\n",
    "        best_score = np.inf\n",
    "        best_x = None\n",
    "        last_num_clusters = 0\n",
    "        for fraction in np.arange(self.min_fraction, self.max_fraction, self.step):\n",
    "            num_clusters = max(floor(fraction * self.num_feats), 3)\n",
    "\n",
    "            if num_clusters == last_num_clusters:\n",
    "                continue\n",
    "\n",
    "            last_num_clusters = num_clusters\n",
    "\n",
    "            x, y = self._cluster_pso(num_clusters)\n",
    "            if y < best_score:\n",
    "                best_score = y\n",
    "                best_x = x\n",
    "\n",
    "        return best_x\n",
    "\n",
    "    def _get_new_df(self, best_particle):\n",
    "        # define a PCA object to combine the clustered\n",
    "        pca = PCA(n_components=1)\n",
    "\n",
    "        # get the clusters out of the particle\n",
    "        clusters = self._get_clusters(best_particle)\n",
    "        # get the cluster\n",
    "        new_dfs = []\n",
    "\n",
    "        for _, feats in clusters.items():\n",
    "            # reduce the clusted features into a single more informative feature\n",
    "            new_feats = pd.DataFrame(data=pca.fit_transform(self.df.loc[:, feats]), index=list(self.df.index))\n",
    "            new_dfs.append(new_feats)\n",
    "\n",
    "        # return the features concatenated horizontally\n",
    "        return pd.concat(new_dfs, axis=1, ignore_index=True)\n",
    "\n",
    "    def eliminate_multicol(self, df: pd.DataFrame):\n",
    "        # first of all determine the vifs of the different columns\n",
    "        vif = self._get_vif(df)\n",
    "        # retrieve multicollinear variables\n",
    "        collinear = list(vif[vif['VIF'] >= self.vif_threshold].index)\n",
    "        collinear_df = df.loc[:, collinear]\n",
    "\n",
    "        # retrieve the non-collinear part\n",
    "        non_collinear = [c for c in df.columns if c not in collinear]\n",
    "        non_collinear_df = df.loc[:, non_collinear]\n",
    "\n",
    "        # if there are no collinear columns, no further preprocessing is needed\n",
    "        if not collinear:\n",
    "            return df\n",
    "\n",
    "        # set the df field to the fraction of the dataframe with only multicollinear columns\n",
    "        self.set_data(collinear_df)\n",
    "        # retrieve the best particle\n",
    "        best_x = self._find_best_cluster()\n",
    "        # retrieve the new representation of the collinear features\n",
    "        new_collinear_df = self._get_new_df(best_x)\n",
    "        # concatenate the two parts to form the final dataframe\n",
    "        return pd.concat([non_collinear_df, new_collinear_df], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "pso = PsoMultiCol()\n",
    "\n",
    "new_dataset = pso.eliminate_multicol(dataset)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(new_dataset, y, test_size=0.2, random_state=11)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_env",
   "language": "python",
   "name": "ds_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
