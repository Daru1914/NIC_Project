{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "this notebook is an implementation of the PSO approach to eliminate multi-collinearily in any given dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_heat_map(data: pd.DataFrame, title: str):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    sns.heatmap(data, linewidth = 1 , annot = True)\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sko.PSO import PSO\n",
    "from math import floor\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "class PsoMultiCol:\n",
    "    def set_data(self, df):\n",
    "        self.df = df\n",
    "        # calculate the spearmanr correlation of the dataframe's features\n",
    "        corr = spearmanr(df).correlation\n",
    "        # make sure it is symmetric\n",
    "        corr = (corr + corr.T) / 2\n",
    "        # fill the diagonal with 1s\n",
    "        np.fill_diagonal(corr, 1)\n",
    "        # transform the matrix to a dataframe that represents how similar each feature it is to another\n",
    "        self.dis_matrix = pd.DataFrame(data= (1 - np.abs (corr)), columns=list(df.columns), index=list(df.columns))\n",
    "        # have a dictionary mapping the column's order to its name\n",
    "        self.columns_dict = dict(list(zip(range(len(df.columns)), df.columns)))\n",
    "        # set the number of features for later reference\n",
    "        self.num_feats = len(df.columns)\n",
    "        # save the column names for later reference\n",
    "        self.columns = list(df.columns)\n",
    "       \n",
    "    def __init__ (self, df:pd.DataFrame=None, max_iter:int= 10, vif_threshold:float=2.5, epsilon:int=0.1,\n",
    "                  min_fraction=0.40, max_fraction=0.76, step=0.05): # add other parameters to the game\n",
    "        self.max_iter = max_iter\n",
    "        self.pso = None\n",
    "        # the value that determine whether columns are multicollinear or not\n",
    "        self.vif_threshold = vif_threshold\n",
    "        # an epsilon value used in the evaluation function\n",
    "        self.epsilon = epsilon\n",
    "        self.min_fraction = min_fraction\n",
    "        self.max_fraction = max_fraction\n",
    "        self.step = step\n",
    "        self.pso = None\n",
    "        if df is not None:\n",
    "            self.set_data(df)\n",
    "    \n",
    "    def _get_vif(self, df=None):\n",
    "        if df is None:\n",
    "            df = self.df\n",
    "        \n",
    "        vif = pd.DataFrame()\n",
    "        vif['VIF'] = [variance_inflation_factor(df.values, i) for i in range(len(df.columns))]\n",
    "        vif['variables'] = df.columns\n",
    "        return vif.set_index('variables')   \n",
    "    \n",
    "    \n",
    "    def _get_clusters(self, particle: np.array):\n",
    "        particle_size = len(particle)\n",
    "        discrete_particle = np.array([int(x) for x in particle])\n",
    "        cluster_feats = {}\n",
    "        for i in range(particle_size) :\n",
    "            # if the value of the cluster is not in the dictinary, initialize the list\n",
    "            if discrete_particle[i] not in cluster_feats:\n",
    "                cluster_feats[discrete_particle[i]] = []\n",
    "            # the cluster_feats will be a map between numbers representing clusters\n",
    "            # and columns representing \n",
    "            cluster_feats[discrete_particle[i]].append(i)\n",
    "        \n",
    "        return cluster_feats    \n",
    "    \n",
    "    def _cluster_scores(self, cluster_feats: dict):\n",
    "        cluster_names = {}\n",
    "        new_order = []\n",
    "        title = \"\"\n",
    "        \n",
    "        # map each cluster to its column names\n",
    "        for c, feats in cluster_feats.items():\n",
    "            cluster_names[c] = [self.columns_dict[i] for i in feats]\n",
    "            new_order.extend(feats)\n",
    "            title += f\"{len(feats)}-\"\n",
    "                        \n",
    "        # how dissimilar the clusters are from each other\n",
    "        inter_cluster_score = 0\n",
    "        keys = list(cluster_names.keys())\n",
    "        n_clusters = len(cluster_feats)\n",
    "        # for i in range(n_clusters):\n",
    "        #     for j in range(i + 1, n_clusters):\n",
    "        #         inter_cluster_score += np.nanmean((self.dis_matrix.loc[cluster_names[keys[i]], cluster_names[keys[j]]]))\n",
    "        \n",
    "        # inter_cluster_score /= (n_clusters) * (n_clusters - 1)\n",
    "\n",
    "        # how similar the points are inside a single cluster \n",
    "        inner_cluster_score = 0\n",
    "        for c, names in cluster_feats.items():\n",
    "            inner_cluster_score += ( 1 + np.exp(self.dis_matrix.iloc[names, names].values.sum())) / np.log(len(names) + np.exp(1))\n",
    "            # inner_cluster_score += 1 + np.nanmean(self.dis_matrix.loc[names, names])         \n",
    "        \n",
    "        # new_dist_matrix = self.dis_matrix.loc[new_order[::-1], new_order]\n",
    "        # display_heat_map(new_dist_matrix, title)\n",
    "        return inner_cluster_score # / inter_cluster_score\n",
    "\n",
    "    def _pso_function(self, particle: np.array):         \n",
    "        return self._cluster_scores(self._get_clusters(particle))\n",
    "    \n",
    "    def _cluster_pso(self, num_clusters):\n",
    "        # determine the function object to pass to the PSO algorithm\n",
    "        pso_function = lambda x: self._pso_function(x)\n",
    "        # bounds\n",
    "        lower_bound = np.zeros(self.num_feats)\n",
    "        upper_bound = np.full(shape=self.num_feats, fill_value = num_clusters, dtype=\"float\")\n",
    "        \n",
    "        pso =  PSO(func=pso_function, n_dim = self.num_feats, pop=15, max_iter=self.max_iter ,lb=lower_bound, ub=upper_bound, c1=1.5, c2=1.5)\n",
    "        pso.run()\n",
    "        \n",
    "        x, y = pso.gbest_x, pso.gbest_y\n",
    "        \n",
    "        cluster_feats = self._get_clusters(x)\n",
    "        \n",
    "        new_order = [] \n",
    "        title = \"\"\n",
    "        for _, f in cluster_feats.items():\n",
    "            new_order.extend(f)\n",
    "            title += f'{len(f)}-'\n",
    "        \n",
    "        # print(y)\n",
    "        # new_dist_matrix = self.dis_matrix.loc[new_order[::-1], new_order]\n",
    "        # display_heat_map(new_dist_matrix, title)\n",
    "        \n",
    "        return x, y\n",
    "    \n",
    "    def _find_best_cluster(self):\n",
    "        best_score = np.inf\n",
    "        best_x = None   \n",
    "        last_num_clusters = 0\n",
    "        for fraction in np.arange(self.min_fraction, self.max_fraction, self.step):            \n",
    "            num_clusters = max(floor(fraction  * self.num_feats), 3)\n",
    "\n",
    "            if num_clusters == last_num_clusters: \n",
    "                continue\n",
    "            \n",
    "            last_num_clusters = num_clusters\n",
    "            \n",
    "            x, y = self._cluster_pso(num_clusters)\n",
    "            if y < best_score:\n",
    "                best_score = y\n",
    "                best_x = x\n",
    "        \n",
    "        return best_x\n",
    "    \n",
    "    def _get_new_df(self, best_particle):\n",
    "        # define a PCA object to combine the clustered \n",
    "        pca = PCA(n_components=1)\n",
    "\n",
    "        # get the clusters out of the particle\n",
    "        clusters = self._get_clusters(best_particle)\n",
    "        # get the cluster\n",
    "        new_dfs = []\n",
    "        \n",
    "        for _, feats in clusters.items():\n",
    "            # reduce the clustered features into a single more informative feature\n",
    "            new_feats = pd.DataFrame(data=pca.fit_transform(self.df.iloc[:, feats]), index=list(self.df.index))\n",
    "            new_dfs.append(new_feats)\n",
    "        \n",
    "        # return the features concatenated horizontally \n",
    "        return pd.concat(new_dfs, axis=1, ignore_index=True)\n",
    "                               \n",
    "    def eliminate_multicol(self, df: pd.DataFrame):\n",
    "        # first of all determine the vifs of the different columns\n",
    "        vif = self._get_vif(df)\n",
    "        # retrieve multicollinear variables\n",
    "        collinear = list(vif[vif['VIF'] >= self.vif_threshold].index)\n",
    "        collinear_df = df.loc[:, collinear]\n",
    "\n",
    "        # retrieve the non-collinear part\n",
    "        non_collinear = [c for c in df.columns if c not in collinear]\n",
    "        non_collinear_df = df.loc[:, non_collinear]\n",
    "        \n",
    "        # if there are no collinear columns, no further preprocessing is needed\n",
    "        if not collinear:\n",
    "            return df\n",
    "        \n",
    "        # set the df field to the fraction of the dataframe with only multicollinear columns\n",
    "        self.set_data(collinear_df)\n",
    "        # retrieve the best particle\n",
    "        best_x = self._find_best_cluster()\n",
    "        # retrieve the new representation of the collinear features\n",
    "        new_collinear_df = self._get_new_df(best_x)\n",
    "        # concatenate the two parts to form the final dataframe\n",
    "        return pd.concat([non_collinear_df, new_collinear_df], axis=1).rename(columns=lambda x: str(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_excel(os.path.join('databases', 'final_dataset.xlsx'), usecols=lambda x: 'Unnamed' not in x)\n",
    "dataset.drop(columns=['name', 'code'], inplace=True)\n",
    "y = dataset.pop('pop_growth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prepare_train_test(dataset, y, test_size=0.2, random_state=11):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(dataset, y, test_size=test_size, random_state=random_state)\n",
    "    X_train.reset_index(inplace=True, drop=True)\n",
    "    X_test.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    # for non year columns\n",
    "    standard_scaler = StandardScaler()\n",
    "    # for the year column\n",
    "    mm_scaler = MinMaxScaler()\n",
    "\n",
    "    X_train['year'] = mm_scaler.fit_transform(X_train.loc[:, ['year']]).reshape(-1, )\n",
    "    scaled_year = X_train.pop('year')\n",
    "\n",
    "    X_train = pd.DataFrame(data=standard_scaler.fit_transform(X_train), columns=X_train.columns)\n",
    "    X_train['year'] = scaled_year\n",
    "\n",
    "    X_test['year'] = mm_scaler.fit_transform(X_test.loc[:, ['year']]).reshape(-1, )\n",
    "    scaled_year = X_test.pop('year')\n",
    "\n",
    "    X_test = pd.DataFrame(data=standard_scaler.fit_transform(X_test), columns=X_test.columns)\n",
    "    X_test['year'] = scaled_year\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset.reset_index(drop=True, inplace=True)\n",
    "X_train, X_test, y_train, y_test = prepare_train_test(dataset, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge_basic = Ridge(max_iter=5000)\n",
    "ridge_grid = {\"alpha\": np.logspace(0.001, 10, 5)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mse': 0.17850173688651289}\n",
      "(DecisionTreeRegressor(max_depth=20, max_features=0.9999999999999999,\n",
      "                      min_samples_leaf=3, min_samples_split=0.02,\n",
      "                      random_state=11), {'mse': 0.6761465793373358})\n"
     ]
    }
   ],
   "source": [
    "from regressors import apply_model\n",
    "ridge_performance = apply_model(ridge_basic, X_train, X_test, y_train, y_test, ridge_grid, save=False)\n",
    "print(ridge_performance[1])\n",
    "\n",
    "\n",
    "decision_tree = DecisionTreeRegressor(random_state=11)\n",
    "\n",
    "dt_params = {\"max_features\": np.arange(0.7, 1.01, 0.1), \"max_depth\": range(10, 21, 2), \n",
    "             \"min_samples_split\":np.arange(0.02, 0.055, 0.01), \"min_samples_leaf\": range(1, 21, 2)}\n",
    "\n",
    "dt_res = apply_model(decision_tree, X_train, X_test, y_train, y_test, dt_params, save=True, save_path=os.path.join(\"decision_tree_1.pkl\"))\n",
    "print(dt_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply multicollinearity elimination\n",
    "pso = PsoMultiCol()\n",
    "\n",
    "new_dataset = pso.eliminate_multicol(dataset)\n",
    "new_dataset.reset_index(drop=True, inplace=True)\n",
    "\n",
    "X_train_new, X_test_new, y_train, y_test = prepare_train_test(new_dataset, y, test_size=0.2, random_state=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ayhem18/.local/lib/python3.10/site-packages/evopreprocess/feature_selection/EvoFeatureSelection.py:164: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  features = stats.mode(features, axis=1, nan_policy='omit')[0].flatten()\n",
      "/tmp/ipykernel_25385/2000718797.py:17: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "  X_new_train.columnes = [str(d) for d in X_new_train.columns]\n"
     ]
    }
   ],
   "source": [
    "from niapy.algorithms.basic import CuckooSearch\n",
    "from evopreprocess.feature_selection import EvoFeatureSelection\n",
    "\n",
    "from Feature_Transformer import FeatureTransformer\n",
    "\n",
    "def use_model(model, grid_params, X_train, X_test, y_train, y_test, save=True, save_path=None):\n",
    "    # use feature selection\n",
    "    evo = EvoFeatureSelection(evaluator=model, random_seed=11, optimizer=CuckooSearch, n_runs=1, n_folds=3)\n",
    "    X_new_train = evo.fit_transform(X_train, y_train)\n",
    "    X_new_test = evo.transform(X_test)\n",
    "    # use feature tranformer\n",
    "    transformer = FeatureTransformer()\n",
    "    X_new_train = transformer.fit_transform(X_new_train, y_train)\n",
    "    X_new_test = transformer.transform(X_new_test)\n",
    "    \n",
    "    return apply_model(model,X_new_train, X_new_test, y_train, y_test, grid_params, save=save, save_path=save_path)[1]\n",
    "\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "decision_tree = DecisionTreeRegressor(random_state=11)\n",
    "\n",
    "dt_params = {\"max_features\": np.arange(0.7, 1.01, 0.1), \"max_depth\": range(10, 21, 2), \n",
    "             \"min_samples_split\":np.arange(0.02, 0.055, 0.01), \"min_samples_leaf\": range(1, 21, 2)}\n",
    "\n",
    "dt_res = use_model(decision_tree, dt_params, X_train_new, X_test_new, y_train, y_test, save=True, save_path=os.path.join(\"decision_tree_2.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mse': 0.7125625988761434}\n"
     ]
    }
   ],
   "source": [
    "print(dt_res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_env",
   "language": "python",
   "name": "ds_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "de47f5c92c0ee6f12a59a5613ac5feff6aab19ddff207ba0b3964cced08c4ccc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
