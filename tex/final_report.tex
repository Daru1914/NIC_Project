\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{minted}
\usepackage{textcomp}
\usepackage{xcolor}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Nature-Inspired Computing\\Project Report\\}

\author{\IEEEauthorblockN{Ayhem Bouabid}
\IEEEauthorblockA{\textit{DS-01} \\
\textit{Innopolis University}\\
Innopolis, Russian Federation \\
a.bouabid@innopolis.university}
\and
\IEEEauthorblockN{Majid Naser}
\IEEEauthorblockA{\textit{DS-01} \\
\textit{Innopolis University}\\
Innopolis, Russian Federation \\
m.naser@innopolis.university}
\and
\IEEEauthorblockN{Nikolay Pavlenko}
\IEEEauthorblockA{\textit{DS-01} \\
\textit{Innopolis University}\\
Innopolis, Russian Federation \\
n.pavlenko@innopolis.university}
}

\maketitle

\begin{abstract}
tbd
\end{abstract}

\begin{IEEEkeywords}
tbd
\end{IEEEkeywords}

\section{Introduction}
Quality data preprocessing is an important prerequisite to creating accurate machine learning models. It solves a wide range of problems, from missing data and data inconsistency to required anonymization. In our project we have decided to focus primarily on one aspect of it, that is, \textbf{feature selection}. \\

Performing feature selection allows us to solve a great number of problems, being especially effective in large datasets containing many features. It improves accuracy of predictions by finding and eliminating spurious relationships, as well as reducing the chances of overfitting. Other effects of feature selection are the improvement of training time for the model through cutting down on unnecessary data, and increase in interpretability, as fewer features have to be analyzed to figure out the dependencies. \\

However, the main problem that can be solved by feature selection for multiple regression models (and will be addressed specifically in our project) is \textbf{multicollinearity}. Multicollinearity is an effect when multiple explanatory variables that are believed to me independent from each other, are in fact, closely interrelated. This can have damaging effects on the accuracy of prediction models, as even a small change in data will lead to unpredictable results. Ideally, feature selections would find such relationships and purge the dataset from them, our model aims to do so as well.\\

As to the practical applications of our project, we have decided to stick to the original project proposal and test it on a new dataset that would contain various countrywide statistics as features that will try to predict the population growth for a country in a given year. Since we have found no such dataset that would have suited our needs, it was created from scratch, using data provided by the World Bank Open Data site\cite{world_bank_moment}. However, during our work on the project, we have decided to also test other similar datasets, that could be used for a multiple regression model, to figure out if our implemented feature selection process can be used more generally.

\textbf{*NEW INFO ABOUT OTHER DATASETS WILL BE ADDED HERE*}

\section{Related Work}

\textbf{*NEW INFO ABOUT SOLVING MULTICOLLINEARITY WILL GO HERE*}

The idea of feature selection using Nature-Inspired Algorithms is not new - we have based our project around already existent frameworks that implement those algorithms and apply them as optimizers in preprocessing tasks.\\

One framework that we have used extensively is NiaPy library\cite{niapy}. They have implemented a large collection of nature-inspired algorithms, and provided a simple interface to use them as optimizers. Their library is also used by the Transaction Fraud Detection project, which is based around the same idea of feature selection with the nature-inspired algorithms and has served as an inspiration behind our own project, so we have decided to follow in their footsteps.\\

Later, in the process of searching for another library that is compatible with NiaPy implementation of various nature-insipred algorithms, and would provide a friendly and extendable Python interface to use them as optimizers in combination with Pandas and sklearn libraries, we have found the evopreprocess library\cite{evopreproc}. \\

That library contains several main modules: \texttt{data\_sampling}, \texttt{data\_weighting}, and \texttt{feature\_selection}. Howeveer, only the last module is relevant for our task, so we will only describe its functionality. The task class for \texttt{feature\_selection} in evopreprocess is \texttt{EvoSampling}, which extends \texttt{\_BaseFilter} class from scikit-learn. It is important to list the parameters of that class, as they are going to play a big role in the feature selection process:
\begin{itemize}
	\item \texttt{random\_seed} - seed state, by default is equal to system time in milliseconds.
	\item \texttt{evaluator} - ML approach used to evaluate the data, expecting a scikit-learn-compatible regressor or classifier.
	\item \texttt{optimizer} - NI optimizer, expecting a NiaPy-copatible method, by default Genetic algorithm
	\item \texttt{n\_folds} - number of folds for the cross-validation split into training and validation sets
	\item \texttt{n\_runs} - number of runs of the optimizer on each fold
	\item \texttt{benchmark} - evaluation class that measures the quality of data sampling. By default optimizes error rate and F-score for classification problems and mean square error for regression problems
	\item \texttt{n\_jobs} - number of optimizers to be run in parallel, by default equal to the number of your CPU's cores
\end{itemize}



\section{Methodology}

In our research, we were able to identify three categories of feature selection methods. Firstly, it is the filter method, which ranks each feature on some statistical metric, and evaluates the ranks afterwards, picking the ones that score the highest. Secondly, it is the wrapper method, which takes a subset of features and trains a model using them. Depending on results of the testing, it adds or removes features from the subset, incrementally improving the performance until user-defined stopping criteria is achieved. Thirdly, it is the embedded method, which is in-built into models themselves - it add a penalizing term to the regression equation (en example of such a model would be Lasso Regression).\\

Among all categories mentioned, wrapper method has the highest computational costs, but can provide the best dataset that would provide the most accurate results for our model. It can also include nature-inspired algorithms as its estimators, making \textbf{wrapped} category of feature selection models our preferred choice. 

\section{Github link}
$https://github.com/Daru1914/NIC\_Project$

\section{Experiments and Evaluation}
tbd

\section{Analysis and Observations}
tbd

\section{Conclusion}
tbd

\bibliographystyle{plain}
\bibliography{referens}

\section{Funni table}

\begin{table}[htbp]
\caption{Table Type Styles}
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Table}&\multicolumn{3}{|c|}{\textbf{Table Column Head}} \\
\cline{2-4} 
\textbf{Head} & \textbf{\textit{Table column subhead}}& \textbf{\textit{Subhead}}& \textbf{\textit{Subhead}} \\
\hline
copy& More table copy$^{\mathrm{a}}$& &  \\
\hline
\multicolumn{4}{l}{$^{\mathrm{a}}$Sample of a Table footnote.}
\end{tabular}
\label{tab1}
\end{center}
\end{table}

\section{Funni picture}

\begin{figure}[htbp]
\centerline{\includegraphics{fig1.png}}
\caption{Example of a figure caption.}
\label{fig}
\end{figure}

\end{document}
