\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Nature-Inspired Computing\\Project Report\\}

\author{\IEEEauthorblockN{Ayhem Bouabid}
\IEEEauthorblockA{\textit{DS-01} \\
\textit{Innopolis University}\\
Innopolis, Russian Federation \\
a.bouabid@innopolis.university}
\and
\IEEEauthorblockN{Majid Naser}
\IEEEauthorblockA{\textit{DS-01} \\
\textit{Innopolis University}\\
Innopolis, Russian Federation \\
m.naser@innopolis.university}
\and
\IEEEauthorblockN{Nikolay Pavlenko}
\IEEEauthorblockA{\textit{DS-01} \\
\textit{Innopolis University}\\
Innopolis, Russian Federation \\
n.pavlenko@innopolis.university}
}

\maketitle

\begin{abstract}
tbd
\end{abstract}

\begin{IEEEkeywords}
tbd
\end{IEEEkeywords}

\section{Introduction}
Quality data preprocessing is an important prerequisite to creating accurate machine learning models. It solves a wide range of problems, from missing data and data inconsistency to required anonymization. In our project we have decided to focus primarily on one aspect of it, that is, \textbf{feature selection}. \\

Performing feature selection allows us to solve a great number of problems, being especially effective in large datasets containing many features. It improves accuracy of predictions by finding and eliminating spurious relationships, as well as reducing the chances of overfitting. Other effects of feature selection are the improvement of training time for the model through cutting down on unnecessary data, and increase in interpretability, as fewer features have to be analyzed to figure out the dependencies. \\

However, the main problem that can be solved by feature selection for multiple regression models (and will be addressed specifically in our project) is \textbf{multicollinearity}. Multicollinearity is an effect when multiple explanatory variables that are believed to me independent from each other, are in fact, closely interrelated. This can have damaging effects on the accuracy of prediction models, as even a small change in data will lead to unpredictable results. Ideally, feature selections would find such relationships and purge the dataset from them, our model aims to do so as well.\\

As to the practical applications of our project, we have decided to stick to the original project proposal and test it on a new dataset that would contain various countrywide statistics as features that will try to predict the population growth for a country in a given year. Since we have found no such dataset that would have suited our needs, it was created from scratch, using data provided by the World Bank Open Data site\cite{world_bank_moment}. However, during our work on the project, we have decided to also test other similar datasets, that could be used for a multiple regression model, to figure out if our implemented feature selection process can be used more generally.

\textbf{*NEW INFO ABOUT OTHER DATASETS WILL BE ADDED HERE*}

\section{Related Work}
tbd

\section{Methodology}

In our research, we were able to identify three categories of feature selection methods. Firstly, it is the filter method, which ranks each feature on some statistical metric, and evaluates the ranks afterwards, picking the ones that score the highest. Secondly, it is the wrapper method, which takes a subset of features and trains a model using them. Depending on results of the testing, it adds or removes features from the subset, incrementally improving the performance until user-defined stopping criteria is achieved. Thirdly, it is the embedded method, which is in-built into models themselves - it add a penalizing term to the regression equation (en example of such a model would be Lasso Regression).\\

Among all categories mentioned, wrapper method has the highest computational costs, but can provide the best dataset that would provide the most accurate results for our model. It can also include nature-inspired algorithms as its estimators, making \textbf{wrapped} category of feature selection models our preferred choice. 

\section{Github link}
$https://github.com/Daru1914/NIC\_Project$

\section{Experiments and Evaluation}
tbd

\section{Analysis and Observations}
tbd

\section{Conclusion}
tbd

\section{References}
\bibliographystyle{plain}
\bibliography{referens}

\section{Funni table}

\begin{table}[htbp]
\caption{Table Type Styles}
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Table}&\multicolumn{3}{|c|}{\textbf{Table Column Head}} \\
\cline{2-4} 
\textbf{Head} & \textbf{\textit{Table column subhead}}& \textbf{\textit{Subhead}}& \textbf{\textit{Subhead}} \\
\hline
copy& More table copy$^{\mathrm{a}}$& &  \\
\hline
\multicolumn{4}{l}{$^{\mathrm{a}}$Sample of a Table footnote.}
\end{tabular}
\label{tab1}
\end{center}
\end{table}

\section{Funni picture}

\begin{figure}[htbp]
\centerline{\includegraphics{fig1.png}}
\caption{Example of a figure caption.}
\label{fig}
\end{figure}

\end{document}
