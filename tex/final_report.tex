\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{minted}
\usepackage{textcomp}
\usepackage{xcolor}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Nature-Inspired Computing\\Project Report\\}

\author{\IEEEauthorblockN{Ayhem Bouabid}
\IEEEauthorblockA{\textit{DS-01} \\
\textit{Innopolis University}\\
Innopolis, Russian Federation \\
a.bouabid@innopolis.university}
\and
\IEEEauthorblockN{Majid Naser}
\IEEEauthorblockA{\textit{DS-01} \\
\textit{Innopolis University}\\
Innopolis, Russian Federation \\
m.naser@innopolis.university}
\and
\IEEEauthorblockN{Nikolay Pavlenko}
\IEEEauthorblockA{\textit{DS-01} \\
\textit{Innopolis University}\\
Innopolis, Russian Federation \\
n.pavlenko@innopolis.university}
}

\maketitle

\begin{abstract}
tbd
\end{abstract}

\begin{IEEEkeywords}
tbd
\end{IEEEkeywords}

\section{Introduction}
Quality data preprocessing is an important prerequisite to creating accurate machine learning models. It solves a wide range of problems, from missing data and data inconsistency to required anonymization. In our project we have decided to focus primarily on one aspect of it, that is, \textbf{feature selection}. \\

Performing feature selection allows us to solve a great number of problems, being especially effective in large datasets containing many features. It improves accuracy of predictions by finding and eliminating spurious relationships, as well as reducing the chances of overfitting. Other effects of feature selection are the improvement of training time for the model through cutting down on unnecessary data, and increase in interpretability, as fewer features have to be analyzed to figure out the dependencies. \\

However, the main problem that can be solved by feature selection for multiple regression models (and will be addressed specifically in our project) is \textbf{multicollinearity}. Multicollinearity is an effect when multiple explanatory variables that are believed to me independent from each other, are in fact, closely interrelated. This can have damaging effects on the accuracy of prediction models, as even a small change in data will lead to unpredictable results. Ideally, feature selections would find such relationships and purge the dataset from them, our model aims to do so as well.\\

As to the practical applications of our project, we have decided to stick to the original project proposal and test it on a new dataset that would contain various countrywide statistics as features that will try to predict the population growth for a country in a given year. Since we have found no such dataset that would have suited our needs, it was created from scratch, using data provided by the World Bank Open Data site\cite{world_bank_moment}. However, during our work on the project, we have decided to also test other similar datasets, that could be used for a multiple regression model, to figure out if our implemented feature selection process can be used more generally.\\

With that goal in mind, during the testing process we have resorted several times to generating new datasets (both for regression and classification problems) with the help of tools provided by the sklearn libraries, with randomly assigned or manually set parameters.

\section{Related Work}

\textbf{*NEW INFO ABOUT SOLVING MULTICOLLINEARITY WILL GO HERE*}

The idea of feature selection using Nature-Inspired Algorithms is not new - we have based our project around already existent frameworks that implement those algorithms and apply them as optimizers in preprocessing tasks.\\

One framework that we have used extensively is NiaPy library\cite{niapy}. They have implemented a large collection of nature-inspired algorithms, and provided a simple interface to use them as optimizers. Their library is also used by the Transaction Fraud Detection project, which is based around the same idea of feature selection with the nature-inspired algorithms and has served as an inspiration behind our own project, so we have decided to follow in their footsteps.\\

Later, in the process of searching for another library that is compatible with NiaPy implementation of various nature-insipred algorithms, and would provide a friendly and extendable Python interface to use them as optimizers in combination with Pandas and sklearn libraries, we have found the evopreprocess library\cite{evopreproc}. \\

That library contains several main modules: \texttt{data\_sampling}, \texttt{data\_weighting}, and \texttt{feature\_selection}. Howeveer, only the last module is relevant for our task, so we will only describe its functionality. The task class for \texttt{feature\_selection} in evopreprocess is \texttt{EvoSampling}, which extends \texttt{\_BaseFilter} class from scikit-learn. It is important to list the parameters of that class, as they are going to play a big role in the feature selection process:
\begin{itemize}
	\item \texttt{random\_seed} - seed state, by default is equal to system time in milliseconds.
	\item \texttt{evaluator} - ML approach used to evaluate the data, expecting a scikit-learn-compatible regressor or classifier.
	\item \texttt{optimizer} - NI optimizer, expecting a NiaPy-copatible method, by default Genetic algorithm
	\item \texttt{n\_folds} - number of folds for the cross-validation split into training and validation sets
	\item \texttt{n\_runs} - number of runs of the optimizer on each fold
	\item \texttt{benchmark} - evaluation class that measures the quality of data sampling. By default optimizes error rate and F-score for classification problems and mean square error for regression problems
	\item \texttt{n\_jobs} - number of optimizers to be run in parallel, by default equal to the number of your CPU's cores
\end{itemize}



\section{Methodology}

\subsection{Multicollinearity issue}

\subsubsection{Multicollinearity detection}

First problem that we are going to address in our dataset would be multicollinearity. It can be detected in a dataset by finding the VIF - variance inflation factor, calculated for each feature separately, according to the formula below, where $R^2$ is the coefficient of determination: 

\begin{equation}
	VIF=\frac{1}{1-R^2}
\end{equation}

VIF can normally take positive values, and by checking its value for each column, one could determine whether or not a serious issue is detected. Value of VIF greater than 1 identifies existence of some collinearity, greater than 5 is already a cause for concern, and greater than 10 must be addressed\cite{menard2002applied}.\\

As our solution to multicollinearity we have decided to combine 2 techniques: clustering and Particle Swarm Algorithm.\\

\subsubsection{Clustering}

When collinear are present in a dataset, model can gather the same information from one feature, as it can from a group of features. This makes it possible to try and solve multicollinearity by applying clustering with a certain threshold on features, that have been proven to have a correlation\cite{sklearnClustering}. In order to find those correlating features, we could use Spearman correlation, which measures the strength and direction of the monotonic relationship between two variables that can be ordered by transforming their values to ranking in their respective domains and comparing their relationship.\\

\subsubsection{Particle Swarm Optimization Clustering}

Particle Swarm Optimization is one of the most influential Nature-Inspired algorithms that can optimize a problem by iteratively improving a chosen solution, depending on the elected performance measure\cite{ballardini2018tutorial}. This property makes it possible to use PSO for our clustering problem, where the final choice of solution will be determined by the algorithm and defined fitness function. The solutions to our optimization problem will be represented as particles, and the aim of the algorithm will be to adjust those particles' position according to the best one found so far, and the best position in the neighborhood of that particle.

\subsubsection{Implementing PSO for Clustering}

PSO can be directly applied to our clustering problem. As was stated before, it can be based on the table containing variables that represent how similar one feature is to another. It is filled according to the formula, where $spr(i, j)$ is the Spearman coefficient between features numbered $i$ and $j$:

\begin{equation}
	x_{i, j}=1-spr(i, j)
\end{equation}

The algorithm will be checking the table to find features that are most closely correlated, and grouping them in clusters, later selecting one of the features from the cluster to explain the influence of all of them onto the target. Ideally, that would remove the multicollinearity problem, and allow us to reduce the number of features in the dataset without significantly harming the predictive capacity of the model.

\subsection{Feature Selection}

In our research, we were able to identify three categories of feature selection methods. Firstly, it is the filter method, which ranks each feature on some statistical metric, and evaluates the ranks afterwards, picking the ones that score the highest. Secondly, it is the wrapper method, which takes a subset of features and trains a model using them. Depending on results of the testing, it adds or removes features from the subset, incrementally improving the performance until user-defined stopping criteria is achieved. Thirdly, it is the embedded method, which is in-built into models themselves - it add a penalizing term to the regression equation (en example of such a model would be Lasso Regression).\\

Among all categories mentioned, wrapper method has the highest computational costs, but can provide the best dataset that would provide the most accurate results for our model. It can also include nature-inspired algorithms as its estimators, making \textbf{wrapped} category of feature selection models our preferred choice. 

\section{Github link}
$https://github.com/Daru1914/NIC\_Project$

\section{Experiments and Evaluation}
tbd

\section{Analysis and Observations}
tbd

\section{Conclusion}
tbd

\bibliographystyle{plain}
\bibliography{referens}

\section{Funni table}

\begin{table}[htbp]
\caption{Table Type Styles}
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Table}&\multicolumn{3}{|c|}{\textbf{Table Column Head}} \\
\cline{2-4} 
\textbf{Head} & \textbf{\textit{Table column subhead}}& \textbf{\textit{Subhead}}& \textbf{\textit{Subhead}} \\
\hline
copy& More table copy$^{\mathrm{a}}$& &  \\
\hline
\multicolumn{4}{l}{$^{\mathrm{a}}$Sample of a Table footnote.}
\end{tabular}
\label{tab1}
\end{center}
\end{table}

\section{Funni picture}

\begin{figure}[htbp]
\centerline{\includegraphics{fig1.png}}
\caption{Example of a figure caption.}
\label{fig}
\end{figure}

\end{document}
